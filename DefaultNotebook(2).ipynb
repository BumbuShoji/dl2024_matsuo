{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m341.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load JSON data from a file and return it as a dictionary\n",
    "def load_json_as_dict(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "file_path = 'VQA train.json'  # Replace with your JSON file path\n",
    "train_dict = load_json_as_dict(file_path)\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "traintrain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers = {}\n",
    "\n",
    "for answers in train_df[\"answers\"]:\n",
    "    for answer in answers:\n",
    "            word = answer[\"answer\"]\n",
    "            if word in all_answers:\n",
    "                all_answers[word] += 1\n",
    "            else:\n",
    "                all_answers[word] = 1\n",
    "                \n",
    "# Convert the dictionary to a DataFrame\n",
    "answers_df = pd.DataFrame(list(all_answers.items()), columns=['Answer', 'Frequency'])\n",
    "\n",
    "answers_df = answers_df.sort_values(by='Frequency', ascending=False)\n",
    "# Display the first 100 rows of the DataFrame\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "answers_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Number of CUDA devices: 1\n",
      "Device 0: Tesla T4\n",
      "Memory Allocated: 0.0 MB\n",
      "Memory Cached: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_cuda_availability():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available.\")\n",
    "        print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"Memory Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 2} MB\")\n",
    "            print(f\"Memory Cached: {torch.cuda.memory_reserved(i) / 1024 ** 2} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_cuda_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "from statistics import mode\n",
    "import os\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.profiler\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_wordnet_synset(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if synsets:\n",
    "        return synsets[0]  # 最初のシノセットを返す\n",
    "    return None\n",
    "\n",
    "def are_semantically_similar(word1, word2):\n",
    "    synset1 = get_wordnet_synset(word1)\n",
    "    synset2 = get_wordnet_synset(word2)\n",
    "    if synset1 and synset2:\n",
    "        similarity = synset1.path_similarity(synset2)\n",
    "        return similarity > 0.5 if similarity else False\n",
    "    return word1 == word2  # シノセットが見つからない場合は文字列比較にフォールバック\n",
    "\n",
    "def map_answer_to_class(answer, class_mapping):\n",
    "    answer = answer.lower()\n",
    "    for class_answer, class_id in class_mapping.items():\n",
    "        if are_semantically_similar(answer, class_answer):\n",
    "            return class_id\n",
    "    return class_mapping.get(\"unanswerable\", len(class_mapping))  # 一致するものがない場合は「unanswerable」クラスを返す\n",
    "\n",
    "\n",
    "\n",
    "# 1. データローダーの作成\n",
    "class VQADataset_preprocessed(torch.utils.data.Dataset):\n",
    "    def __init__(self, processed_data_path, preprocessed_dir, answer=True):\n",
    "        self.df = pd.read_json(processed_data_path)\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "        self.answer = answer\n",
    "        self.answer_to_class_id = {}\n",
    "        self.class_id_to_answer = {}\n",
    "       \n",
    "        if self.answer:\n",
    "            self._create_answer_mapping()\n",
    "            \n",
    "    def _create_answer_mapping(self):\n",
    "        for answers in self.df[\"answers\"]:\n",
    "            for answer in answers:\n",
    "                word = process_text(answer[\"answer\"])\n",
    "                if word not in self.answer_to_class_id:\n",
    "                    class_id = len(self.answer_to_class_id)\n",
    "                    self.answer_to_class_id[word] = class_id\n",
    "                    self.class_id_to_answer[class_id] = word\n",
    "\n",
    "        # Add 'unanswerable' class if not present\n",
    "        if \"unanswerable\" not in self.answer_to_class_id:\n",
    "            unknown_class_id = len(self.answer_to_class_id)\n",
    "            self.answer_to_class_id[\"unanswerable\"] = unknown_class_id\n",
    "            self.class_id_to_answer[unknown_class_id] = \"unanswerable\"\n",
    "\n",
    "    def update_answer_mapping(self, other_dataset):\n",
    "        for word, class_id in other_dataset.answer_to_class_id.items():\n",
    "            if word not in self.answer_to_class_id:\n",
    "                self.answer_to_class_id[word] = class_id\n",
    "                self.class_id_to_answer[class_id] = word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(os.path.join(self.preprocessed_dir, self.df['processed_image_path'][idx]))\n",
    "        question_embedding = torch.load(os.path.join(self.preprocessed_dir, self.df['processed_question_path'][idx]))\n",
    "\n",
    "        if self.answer:\n",
    "            answers = [self.answer_to_class_id[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
    "            mode_answer_idx = mode(answers)\n",
    "            return image, question_embedding, torch.Tensor(answers), int(mode_answer_idx)\n",
    "        else:\n",
    "            return image, question_embedding\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, tokenizer, model,transform=None, answer=True):\n",
    "        self.transform = transform  # 画像の前処理\n",
    "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
    "        self.df = pd.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
    "        self.answer = answer\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "        self.answer_to_class_id = {}\n",
    "        self.class_id_to_answer = {}\n",
    "       \n",
    "        # Initialize answer dictionary if answer is True\n",
    "    \n",
    "        if self.answer:\n",
    "            self._create_answer_mapping()\n",
    "\n",
    "    def _create_answer_mapping(self):\n",
    "        for answers in self.df[\"answers\"]:\n",
    "            for answer in answers:\n",
    "                word = process_text(answer[\"answer\"])\n",
    "                if word not in self.answer_to_class_id:\n",
    "                    class_id = len(self.answer_to_class_id)\n",
    "                    self.answer_to_class_id[word] = class_id\n",
    "                    self.class_id_to_answer[class_id] = word\n",
    "\n",
    "        # Add 'unanswerable' class if not present\n",
    "        if \"unanswerable\" not in self.answer_to_class_id:\n",
    "            unknown_class_id = len(self.answer_to_class_id)\n",
    "            self.answer_to_class_id[\"unanswerable\"] = unknown_class_id\n",
    "            self.class_id_to_answer[unknown_class_id] = \"unanswerable\"\n",
    "\n",
    "    def update_answer_mapping(self, other_dataset):\n",
    "        for word, class_id in other_dataset.answer_to_class_id.items():\n",
    "            if word not in self.answer_to_class_id:\n",
    "                self.answer_to_class_id[word] = class_id\n",
    "                self.class_id_to_answer[class_id] = word\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        question = process_text(self.df[\"question\"][idx])\n",
    "    \n",
    "        inputs = self.tokenizer(question, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        question_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "        if self.answer:\n",
    "            answers = [self.answer_to_class_id[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
    "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
    "\n",
    "            return image, question_embedding, torch.Tensor(answers), int(mode_answer_idx)\n",
    "\n",
    "        else:\n",
    "            return image, question_embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. モデルのの実装\n",
    "# ResNetを利用できるようにしておく\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, layers[0], 64)\n",
    "        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, 512)\n",
    "\n",
    "    def _make_layer(self, block, blocks, out_channels, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3])\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, n_answer: int):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet18()\n",
    "\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, n_answer)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, question_embedding):\n",
    "        image_feature = self.resnet(image)  # 画像の特徴量\n",
    "\n",
    "        x = torch.cat([image_feature, question_embedding], dim=1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# 2. 評価指標の実装\n",
    "# 簡単にするならBCEを利用する\n",
    "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
    "    total_acc = 0.\n",
    "\n",
    "    for pred, answers in zip(batch_pred, batch_answers):\n",
    "        acc = 0.\n",
    "        for i in range(len(answers)):\n",
    "            num_match = 0\n",
    "            for j in range(len(answers)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if pred == answers[j]:\n",
    "                    num_match += 1\n",
    "            acc += min(num_match / 3, 1)\n",
    "        total_acc += acc / 10\n",
    "\n",
    "    return total_acc / len(batch_pred)\n",
    "\n",
    "# 4. 学習の実装\n",
    "# Training and Evaluation Functions\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for image, question_embedding, answers, mode_answer in dataloader:\n",
    "        image, question_embedding, answers, mode_answer = \\\n",
    "            image.to(device), question_embedding.to(device), answers.to(device), mode_answer.to(device)\n",
    "\n",
    "        pred = model(image, question_embedding)\n",
    "        loss = criterion(pred, mode_answer.squeeze())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
    "    #return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
    "\n",
    "\n",
    "def eval(model, dataloader, optimizer, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for image, question_embedding, answers, mode_answer in dataloader:\n",
    "        image, question_embedding, answers, mode_answer = \\\n",
    "            image.to(device), question_embedding.to(device), answers.to(device), mode_answer.to(device)\n",
    "\n",
    "        pred = model(image, question_embedding)\n",
    "        loss = criterion(pred, mode_answer.squeeze())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).mean().item()\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = 10**8\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss == 10**8:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(f\"Early stopping triggered. Best Loss: {self.best_loss}\")\n",
    "                    \n",
    "                    \n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "def train_with_profiler(model, dataloader, optimizer, criterion, device, num_epochs=1):\n",
    "    model.train()\n",
    "    \n",
    "    # プロファイラの設定を最新のものに更新\n",
    "    with torch.profiler.profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=4, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/profiler'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as profiler:\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (image, question_embedding, answers, mode_answer) in enumerate(dataloader):\n",
    "                image, question_embedding, answers, mode_answer = \\\n",
    "                    image.to(device), question_embedding.to(device), answers.to(device), mode_answer.to(device)\n",
    "                \n",
    "                with record_function(\"model_forward\"):\n",
    "                    pred = model(image, question_embedding)\n",
    "                    loss = criterion(pred, mode_answer.squeeze())\n",
    "                \n",
    "                with record_function(\"backward\"):\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                \n",
    "                with record_function(\"optimizer_step\"):\n",
    "                    optimizer.step()\n",
    "                \n",
    "                profiler.step()\n",
    "                \n",
    "                #if batch_idx >= 100:  # プロファイリングを早めに終了\n",
    "                #    break\n",
    "\n",
    "    # プロファイラの結果を表示\n",
    "    print(profiler.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    print(profiler.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ファイルが存在します。\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/your/directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ディレクトリの内容をリスト表示\u001b[39;00m\n\u001b[1;32m     13\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/your/directory\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mディレクトリの内容:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/directory'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ファイルのパスを指定\n",
    "file_path = '/workspace/preprocessed_test/processed_data.json'\n",
    "\n",
    "# ファイルの存在を確認\n",
    "if os.path.exists(file_path):\n",
    "    print(\"ファイルが存在します。\")\n",
    "else:\n",
    "    print(\"ファイルが見つかりません。パスを再確認してください。\")\n",
    "\n",
    "# ディレクトリの内容をリスト表示\n",
    "directory_path = '/path/to/your/directory'\n",
    "print(\"ディレクトリの内容:\", os.listdir(directory_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-07-15 07:14:49 133:133 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-07-15 07:14:53 133:133 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-07-15 07:14:53 133:133 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.06%       2.327ms        79.21%        2.964s     740.948ms       0.000us         0.00%        1.011s     252.739ms           0 b           0 b      -1.50 Mb    -591.05 Mb             4  \n",
      "                                               aten::to         0.03%       1.058ms        73.40%        2.747s       6.077ms       0.000us         0.00%      50.392ms     111.487us       1.36 Kb          72 b     591.05 Mb           0 b           452  \n",
      "                                         aten::_to_copy         0.07%       2.436ms        73.38%        2.746s       7.461ms       0.000us         0.00%      50.392ms     136.935us       1.37 Kb         288 b     591.05 Mb           0 b           368  \n",
      "                                            aten::copy_         0.02%     702.000us        73.30%        2.743s       7.453ms      50.462ms         1.31%      50.462ms     137.125us           0 b           0 b           0 b           0 b           368  \n",
      "                                  cudaStreamSynchronize        73.26%        2.741s        73.27%        2.742s     171.350ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            16  \n",
      "                                  cudaDeviceSynchronize        18.48%     691.665ms        18.48%     691.665ms     691.665ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                               backward         2.61%      97.771ms         2.67%      99.768ms      24.942ms       0.000us         0.00%       8.000us       2.000us           0 b           0 b     -20.75 Gb     -20.25 Gb             4  \n",
      "                                          model_forward         0.49%      18.204ms         1.63%      61.001ms      15.250ms       0.000us         0.00%     910.071ms     227.518ms           0 b           0 b      20.74 Gb      -1.09 Gb             4  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.05%       2.021ms         1.54%      57.519ms     718.987us       0.000us         0.00%        1.626s      20.331ms           0 b           0 b      -5.86 Gb     -13.78 Gb            80  \n",
      "                                   ConvolutionBackward0         0.03%       1.018ms         1.46%      54.627ms     682.837us       0.000us         0.00%        1.581s      19.761ms           0 b           0 b       7.92 Gb           0 b            80  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.742s\n",
      "Self CUDA time total: 3.844s\n",
      "\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.05%       2.021ms         1.54%      57.519ms     718.987us       0.000us         0.00%        1.626s      20.331ms           0 b           0 b      -5.86 Gb     -13.78 Gb            80  \n",
      "                                   ConvolutionBackward0         0.03%       1.018ms         1.46%      54.627ms     682.837us       0.000us         0.00%        1.581s      19.761ms           0 b           0 b       7.92 Gb           0 b            80  \n",
      "                             aten::convolution_backward         0.72%      27.054ms         1.43%      53.609ms     670.112us        1.539s        40.02%        1.581s      19.761ms           0 b           0 b       7.92 Gb       7.75 Gb            80  \n",
      "                                          ProfilerStep*         0.06%       2.327ms        79.21%        2.964s     740.948ms       0.000us         0.00%        1.011s     252.739ms           0 b           0 b      -1.50 Mb    -591.05 Mb             4  \n",
      "                                          model_forward         0.49%      18.204ms         1.63%      61.001ms      15.250ms       0.000us         0.00%     910.071ms     227.518ms           0 b           0 b      20.74 Gb      -1.09 Gb             4  \n",
      "                                      aten::convolution         0.04%       1.680ms         0.43%      16.209ms     202.613us       0.000us         0.00%     597.328ms       7.467ms           0 b           0 b       9.47 Gb           0 b            80  \n",
      "                                     aten::_convolution         0.05%       1.833ms         0.39%      14.529ms     181.613us       0.000us         0.00%     597.328ms       7.467ms           0 b           0 b       9.47 Gb           0 b            80  \n",
      "                                           aten::conv2d         0.03%       1.028ms         0.45%      16.838ms     210.475us       0.000us         0.00%     581.921ms       7.274ms           0 b           0 b       9.47 Gb     269.50 Mb            80  \n",
      "                                aten::cudnn_convolution         0.19%       7.121ms         0.26%       9.851ms     123.138us     511.663ms        13.31%     511.663ms       6.396ms           0 b           0 b       9.47 Gb       9.47 Gb            80  \n",
      "                                  volta_sgemm_128x64_nt         0.00%       0.000us         0.00%       0.000us       0.000us     369.647ms         9.62%     369.647ms       3.360ms           0 b           0 b           0 b           0 b           110  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.742s\n",
      "Self CUDA time total: 3.844s\n",
      "\n",
      "【1/50】\n",
      "train time: 145.37 [s]\n",
      "train loss: 5.3575\n",
      "train acc: 0.4779\n",
      "train simple acc: 0.3888\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 5.3575\n",
      "【2/50】\n",
      "train time: 138.55 [s]\n",
      "train loss: 5.0422\n",
      "train acc: 0.4806\n",
      "train simple acc: 0.3922\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 5.0422\n",
      "【3/50】\n",
      "train time: 132.39 [s]\n",
      "train loss: 4.7299\n",
      "train acc: 0.4884\n",
      "train simple acc: 0.3994\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 4.7299\n",
      "【4/50】\n",
      "train time: 265.28 [s]\n",
      "train loss: 4.4352\n",
      "train acc: 0.4924\n",
      "train simple acc: 0.4029\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 4.4352\n",
      "【5/50】\n",
      "train time: 467.34 [s]\n",
      "train loss: 4.1269\n",
      "train acc: 0.4935\n",
      "train simple acc: 0.4053\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 4.1269\n",
      "【6/50】\n",
      "train time: 416.58 [s]\n",
      "train loss: 3.7843\n",
      "train acc: 0.5001\n",
      "train simple acc: 0.4130\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 3.7843\n",
      "【7/50】\n",
      "train time: 421.76 [s]\n",
      "train loss: 3.4496\n",
      "train acc: 0.5022\n",
      "train simple acc: 0.4187\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 3.4496\n",
      "【8/50】\n",
      "train time: 481.93 [s]\n",
      "train loss: 3.1545\n",
      "train acc: 0.5032\n",
      "train simple acc: 0.4198\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 3.1545\n",
      "【9/50】\n",
      "train time: 432.19 [s]\n",
      "train loss: 2.8918\n",
      "train acc: 0.5162\n",
      "train simple acc: 0.4391\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 2.8918\n",
      "【10/50】\n",
      "train time: 329.80 [s]\n",
      "train loss: 2.6896\n",
      "train acc: 0.5242\n",
      "train simple acc: 0.4533\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 2.6896\n",
      "【11/50】\n",
      "train time: 326.09 [s]\n",
      "train loss: 2.5201\n",
      "train acc: 0.5397\n",
      "train simple acc: 0.4750\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 2.5201\n",
      "【12/50】\n",
      "train time: 325.78 [s]\n",
      "train loss: 2.3802\n",
      "train acc: 0.5525\n",
      "train simple acc: 0.4939\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 2.3802\n",
      "【13/50】\n",
      "train time: 320.78 [s]\n",
      "train loss: 2.2789\n",
      "train acc: 0.5629\n",
      "train simple acc: 0.5092\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 2.2789\n",
      "【14/50】\n",
      "train time: 322.39 [s]\n",
      "train loss: 2.1785\n",
      "train acc: 0.5767\n",
      "train simple acc: 0.5289\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 2.1785\n",
      "【15/50】\n",
      "train time: 319.97 [s]\n",
      "train loss: 2.0722\n",
      "train acc: 0.5878\n",
      "train simple acc: 0.5430\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 2.0722\n",
      "【16/50】\n",
      "train time: 318.48 [s]\n",
      "train loss: 1.9893\n",
      "train acc: 0.5925\n",
      "train simple acc: 0.5521\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.9893\n",
      "【17/50】\n",
      "train time: 319.54 [s]\n",
      "train loss: 1.9254\n",
      "train acc: 0.6034\n",
      "train simple acc: 0.5668\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.9254\n",
      "【18/50】\n",
      "train time: 318.80 [s]\n",
      "train loss: 1.8550\n",
      "train acc: 0.6140\n",
      "train simple acc: 0.5790\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.8550\n",
      "【19/50】\n",
      "train time: 322.08 [s]\n",
      "train loss: 1.8189\n",
      "train acc: 0.6136\n",
      "train simple acc: 0.5802\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.8189\n",
      "【20/50】\n",
      "train time: 315.19 [s]\n",
      "train loss: 1.7330\n",
      "train acc: 0.6263\n",
      "train simple acc: 0.5969\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.7330\n",
      "【21/50】\n",
      "train time: 315.50 [s]\n",
      "train loss: 1.6646\n",
      "train acc: 0.6367\n",
      "train simple acc: 0.6089\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.6646\n",
      "【22/50】\n",
      "train time: 313.62 [s]\n",
      "train loss: 1.6167\n",
      "train acc: 0.6426\n",
      "train simple acc: 0.6169\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.6167\n",
      "【23/50】\n",
      "train time: 313.75 [s]\n",
      "train loss: 1.5472\n",
      "train acc: 0.6522\n",
      "train simple acc: 0.6297\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.5472\n",
      "【24/50】\n",
      "train time: 314.79 [s]\n",
      "train loss: 1.5007\n",
      "train acc: 0.6569\n",
      "train simple acc: 0.6365\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.5007\n",
      "【25/50】\n",
      "train time: 310.44 [s]\n",
      "train loss: 1.4488\n",
      "train acc: 0.6644\n",
      "train simple acc: 0.6447\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.4488\n",
      "【26/50】\n",
      "train time: 313.34 [s]\n",
      "train loss: 1.4046\n",
      "train acc: 0.6691\n",
      "train simple acc: 0.6517\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.4046\n",
      "【27/50】\n",
      "train time: 311.45 [s]\n",
      "train loss: 1.3367\n",
      "train acc: 0.6795\n",
      "train simple acc: 0.6652\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.3367\n",
      "【28/50】\n",
      "train time: 310.30 [s]\n",
      "train loss: 1.2919\n",
      "train acc: 0.6861\n",
      "train simple acc: 0.6735\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.2919\n",
      "【29/50】\n",
      "train time: 310.70 [s]\n",
      "train loss: 1.2338\n",
      "train acc: 0.6958\n",
      "train simple acc: 0.6864\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.2338\n",
      "【30/50】\n",
      "train time: 311.76 [s]\n",
      "train loss: 1.1949\n",
      "train acc: 0.7001\n",
      "train simple acc: 0.6922\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.1949\n",
      "【31/50】\n",
      "train time: 310.48 [s]\n",
      "train loss: 1.1408\n",
      "train acc: 0.7088\n",
      "train simple acc: 0.7027\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.1408\n",
      "【32/50】\n",
      "train time: 334.35 [s]\n",
      "train loss: 1.0990\n",
      "train acc: 0.7112\n",
      "train simple acc: 0.7069\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.0990\n",
      "【33/50】\n",
      "train time: 335.19 [s]\n",
      "train loss: 1.0581\n",
      "train acc: 0.7218\n",
      "train simple acc: 0.7187\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.0581\n",
      "【34/50】\n",
      "train time: 312.42 [s]\n",
      "train loss: 1.0181\n",
      "train acc: 0.7302\n",
      "train simple acc: 0.7303\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 1.0181\n",
      "【35/50】\n",
      "train time: 309.04 [s]\n",
      "train loss: 0.9755\n",
      "train acc: 0.7358\n",
      "train simple acc: 0.7377\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.9755\n",
      "【36/50】\n",
      "train time: 305.02 [s]\n",
      "train loss: 0.9171\n",
      "train acc: 0.7453\n",
      "train simple acc: 0.7501\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.9171\n",
      "【37/50】\n",
      "train time: 310.68 [s]\n",
      "train loss: 0.8781\n",
      "train acc: 0.7523\n",
      "train simple acc: 0.7577\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.8781\n",
      "【38/50】\n",
      "train time: 307.03 [s]\n",
      "train loss: 0.8461\n",
      "train acc: 0.7602\n",
      "train simple acc: 0.7675\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.8461\n",
      "【39/50】\n",
      "train time: 303.62 [s]\n",
      "train loss: 0.7939\n",
      "train acc: 0.7710\n",
      "train simple acc: 0.7812\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.7939\n",
      "【40/50】\n",
      "train time: 300.43 [s]\n",
      "train loss: 0.7664\n",
      "train acc: 0.7756\n",
      "train simple acc: 0.7875\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.7664\n",
      "【41/50】\n",
      "train time: 299.59 [s]\n",
      "train loss: 0.7308\n",
      "train acc: 0.7804\n",
      "train simple acc: 0.7941\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.7308\n",
      "【42/50】\n",
      "train time: 300.84 [s]\n",
      "train loss: 0.6905\n",
      "train acc: 0.7875\n",
      "train simple acc: 0.8025\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.6905\n",
      "【43/50】\n",
      "train time: 299.82 [s]\n",
      "train loss: 0.6412\n",
      "train acc: 0.7987\n",
      "train simple acc: 0.8174\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.6412\n",
      "【44/50】\n",
      "train time: 297.28 [s]\n",
      "train loss: 0.6060\n",
      "train acc: 0.8062\n",
      "train simple acc: 0.8272\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.6060\n",
      "【45/50】\n",
      "train time: 296.62 [s]\n",
      "train loss: 0.5776\n",
      "train acc: 0.8142\n",
      "train simple acc: 0.8372\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.5776\n",
      "【46/50】\n",
      "train time: 296.35 [s]\n",
      "train loss: 0.5274\n",
      "train acc: 0.8231\n",
      "train simple acc: 0.8477\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.5274\n",
      "【47/50】\n",
      "train time: 294.11 [s]\n",
      "train loss: 0.4920\n",
      "train acc: 0.8314\n",
      "train simple acc: 0.8580\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.4920\n",
      "【48/50】\n",
      "train time: 295.26 [s]\n",
      "train loss: 0.4499\n",
      "train acc: 0.8412\n",
      "train simple acc: 0.8698\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.4499\n",
      "【49/50】\n",
      "train time: 293.12 [s]\n",
      "train loss: 0.4486\n",
      "train acc: 0.8385\n",
      "train simple acc: 0.8679\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.4486\n",
      "【50/50】\n",
      "train time: 293.33 [s]\n",
      "train loss: 0.3949\n",
      "train acc: 0.8528\n",
      "train simple acc: 0.8846\n",
      "Current learning rate: 0.001000\n",
      "New best model saved with loss: 0.3949\n",
      "Loading best model with loss: 0.3949\n",
      "Submission file created with best model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # デバイスの設定\n",
    "    set_seed(42)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 前処理済みデータのパス\n",
    "    preprocessed_train_path = \"/workspace/preprocessed_train/processed_data.json\"\n",
    "    preprocessed_test_path = \"/workspace/preprocessed_test/processed_data.json\"\n",
    "\n",
    "    # データセットの準備\n",
    "    train_dataset = VQADataset_preprocessed(processed_data_path=preprocessed_train_path, preprocessed_dir=\"/workspace/preprocessed_train\")\n",
    "    test_dataset = VQADataset_preprocessed(processed_data_path=preprocessed_test_path, preprocessed_dir=\"/workspace/preprocessed_test\", answer=False)\n",
    "    \n",
    "    # テストデータセットの回答マッピングをトレインデータセットから引き継ぐ\n",
    "    test_dataset.update_answer_mapping(train_dataset)\n",
    "\n",
    "    # データローダーの作成\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # モデルの初期化\n",
    "    vqa_model = VQAModel(n_answer=len(train_dataset.answer_to_class_id)).to(device)\n",
    "\n",
    "    # オプティマイザと損失関数\n",
    "    num_epoch = 50\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(vqa_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    # EarlyStoppingの初期化\n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "    # モデルの保存先ディレクトリ\n",
    "    model_dir = 'saved_models'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # 訓練履歴を保存するリスト\n",
    "    train_history = []\n",
    "    \n",
    "    #TRAIN profiler\n",
    "    train_with_profiler(vqa_model, train_loader, optimizer, criterion, device, num_epochs=1)\n",
    "    \n",
    "    # トレーニング開始\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss, train_acc, train_simple_acc, train_time = train(vqa_model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        # 訓練結果を保存\n",
    "        train_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': train_loss,\n",
    "            'acc': train_acc,\n",
    "            'simple_acc': train_simple_acc,\n",
    "            'time': train_time\n",
    "        })\n",
    "        \n",
    "        print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
    "              f\"train time: {train_time:.2f} [s]\\n\"\n",
    "              f\"train loss: {train_loss:.4f}\\n\"\n",
    "              f\"train acc: {train_acc:.4f}\\n\"\n",
    "              f\"train simple acc: {train_simple_acc:.4f}\\n\"\n",
    "              f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping と最良モデルの保存\n",
    "        if train_loss < early_stopping.best_loss:\n",
    "            early_stopping.best_loss = train_loss\n",
    "            early_stopping.counter = 0\n",
    "            torch.save(vqa_model.state_dict(), os.path.join(model_dir, \"best_model.pth\"))\n",
    "            print(f\"New best model saved with loss: {early_stopping.best_loss:.4f}\")\n",
    "        else:\n",
    "            early_stopping.counter += 1\n",
    "            if early_stopping.counter >= early_stopping.patience:\n",
    "                print(f\"Early stopping triggered. No improvement for {early_stopping.patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    # 訓練履歴を保存\n",
    "    with open('train_history.json', 'w') as f:\n",
    "        json.dump(train_history, f)\n",
    "\n",
    "    # 最良モデルを読み込む\n",
    "    print(f\"Loading best model with loss: {early_stopping.best_loss:.4f}\")\n",
    "    vqa_model.load_state_dict(torch.load(os.path.join(model_dir, \"best_model.pth\")))\n",
    "\n",
    "    # 提出用ファイルの作成\n",
    "    vqa_model.eval()\n",
    "    submission = []\n",
    "    with torch.no_grad():\n",
    "        for image, question in test_loader:\n",
    "            image, question = image.to(device), question.to(device)\n",
    "            pred = vqa_model(image, question)\n",
    "            pred_class_ids = pred.argmax(1).cpu().tolist()\n",
    "            submission.extend([train_dataset.class_id_to_answer[class_id] for class_id in pred_class_ids])\n",
    "\n",
    "    submission = np.array(submission)\n",
    "    np.save(\"submission.npy\", submission)\n",
    "    print(\"Submission file created with best model.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VQADataset.__init__() got an unexpected keyword argument 'df_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmission file created with best model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m DistilBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# データセットの準備\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mVQADataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_path_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_dir_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m VQADataset(df_path\u001b[38;5;241m=\u001b[39mdf_path_test, image_dir\u001b[38;5;241m=\u001b[39mimage_dir_test, transform\u001b[38;5;241m=\u001b[39mtransform, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, model\u001b[38;5;241m=\u001b[39mmodel, answer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# テストデータセットの回答マッピングをトレインデータセットから引き継ぐ\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: VQADataset.__init__() got an unexpected keyword argument 'df_path'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # デバイスの設定\n",
    "    set_seed(42)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # データ変換\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # データパス\n",
    "    df_path_train = \"VQA train.json\"\n",
    "    image_dir_train = \"train\"\n",
    "    df_path_test = \"VQA valid.json\"\n",
    "    image_dir_test = \"valid\"\n",
    "    \n",
    "    # トークナイザーとモデルのロード\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    # データセットの準備\n",
    "    train_dataset = VQADataset(df_path=df_path_train, image_dir=image_dir_train, transform=transform, tokenizer=tokenizer, model=model)\n",
    "    test_dataset = VQADataset(df_path=df_path_test, image_dir=image_dir_test, transform=transform, tokenizer=tokenizer, model=model, answer=False)\n",
    "    \n",
    "    # テストデータセットの回答マッピングをトレインデータセットから引き継ぐ\n",
    "    test_dataset.update_answer_mapping(train_dataset)\n",
    "\n",
    "    # データローダーの作成\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2, prefetch_factor=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, prefetch_factor=2)\n",
    "\n",
    "    # モデルの初期化\n",
    "    vqa_model = VQAModel(n_answer=len(train_dataset.answer_to_class_id)).to(device)\n",
    "\n",
    "    # オプティマイザと損失関数\n",
    "    num_epoch = 50\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(vqa_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    # EarlyStoppingの初期化\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    # モデルの保存先ディレクトリ\n",
    "    model_dir = 'saved_models'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # 訓練履歴を保存するリスト\n",
    "    train_history = []\n",
    "\n",
    "    # トレーニング開始\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        for epoch in range(num_epoch):\n",
    "            train_loss, train_acc, train_simple_acc, train_time = train(vqa_model, train_loader, optimizer, criterion, device)\n",
    "            \n",
    "            scheduler.step(train_loss)\n",
    "\n",
    "            # 訓練結果を保存\n",
    "            train_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'loss': train_loss,\n",
    "                'acc': train_acc,\n",
    "                'simple_acc': train_simple_acc,\n",
    "                'time': train_time\n",
    "            })\n",
    "            \n",
    "            print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
    "                  f\"train time: {train_time:.2f} [s]\\n\"\n",
    "                  f\"train loss: {train_loss:.4f}\\n\"\n",
    "                  f\"train acc: {train_acc:.4f}\\n\"\n",
    "                  f\"train simple acc: {train_simple_acc:.4f}\\n\"\n",
    "                  f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Early stopping と最良モデルの保存\n",
    "            if train_loss < early_stopping.best_loss:\n",
    "                early_stopping.best_loss = train_loss\n",
    "                early_stopping.counter = 0\n",
    "                torch.save(vqa_model.state_dict(), os.path.join(model_dir, \"best_model.pth\"))\n",
    "                print(f\"New best model saved with loss: {early_stopping.best_loss:.4f}\")\n",
    "            else:\n",
    "                early_stopping.counter += 1\n",
    "                if early_stopping.counter >= early_stopping.patience:\n",
    "                    print(f\"Early stopping triggered. No improvement for {early_stopping.patience} epochs.\")\n",
    "                    break\n",
    "            \n",
    "            prof.step()\n",
    "            \n",
    "\n",
    "    # 訓練履歴を保存\n",
    "    with open('train_history.json', 'w') as f:\n",
    "        json.dump(train_history, f)\n",
    "\n",
    "    # 最良モデルを読み込む\n",
    "    print(f\"Loading best model with loss: {early_stopping.best_loss:.4f}\")\n",
    "    vqa_model.load_state_dict(torch.load(os.path.join(model_dir, \"best_model.pth\")))\n",
    "\n",
    "    # 提出用ファイルの作成\n",
    "    vqa_model.eval()\n",
    "    submission = []\n",
    "    with torch.no_grad():\n",
    "        for image, question in test_loader:\n",
    "            image, question = image.to(device), question.to(device)\n",
    "            pred = vqa_model(image, question)\n",
    "            pred_class_id = pred.argmax(1).cpu().item()\n",
    "            submission.append(train_dataset.class_id_to_answer[pred_class_id])\n",
    "\n",
    "    submission = np.array(submission)\n",
    "    np.save(\"submission.npy\", submission)\n",
    "    print(\"Submission file created with best model.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 427411,
     "sourceId": 813452,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5223331,
     "sourceId": 8707791,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5223855,
     "sourceId": 8708590,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 24264092,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
